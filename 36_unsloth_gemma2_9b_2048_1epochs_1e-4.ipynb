{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMSYS Chatbot Arena - Training Notebook\n",
    " \n",
    "| **Competition** | [LMSYS Chatbot Arena on Kaggle](https://www.kaggle.com/competitions/lmsys-chatbot-arena) |\n",
    "|-------|-------|\n",
    "| **Author** | Guillaume Raille ([grll](https://github.com/grll)) |\n",
    "| **Date** | 2024-09-17 |\n",
    "\n",
    "This notebook demonstrates how to efficiently fine-tune the Gemma-2-9b model for sequence classification using QLoRA with the Unsloth library in the context of the LMSYS Chatbot Arena competition. QLoRA and Unsloth enable to fine-tune LLM with much lower GPU compute and memory requirements for minimal performance impact.\n",
    "\n",
    "The objective is to classify sequences composed of: a prompt, a response from model A, a response from model B into: model A wins, model B wins or it's a tie based on human preference. \n",
    "\n",
    "We will cover data preparation, model configuration and training.\n",
    "\n",
    "This notebook was executed during the competition on a machine with a single RTX4090 GPU from [vast.ai](https://vast.ai/). It took ~5 hours to run at a cost of ~0.3 USD/hour so a total of ~1.5 USD were spent on this experiment.\n",
    "\n",
    "The model obtained in this notebook with no additional data or tricks except quantization with autoAWQ for efficient inference achieved a ranking of 115th out of 1849 in the competition.\n",
    "\n",
    "The training dataset to reproduce the experiment can be found on [kaggle](https://www.kaggle.com/competitions/lmsys-chatbot-arena)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187caa8f",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [1. Setup](#1-setup)\n",
    "2. [2. Data Transformations](#2-data-transformations)\n",
    "    1. [2.1 RawParser](#21-rawparser)\n",
    "    2. [2.2 ParsedTokenizer](#22-parsedtokenizer)\n",
    "    3. [2.3 SampleCreator](#23-samplecreator)\n",
    "    4. [2.4 LabelCreator](#24-labelcreator)\n",
    "3. [3. Configuration](#3-configuration)\n",
    "4. [4. Data Preparation](#4-data-preparation)\n",
    "    1. [4.0 Load the model and tokenizer](#40-load-the-model-and-tokenizer)\n",
    "    2. [4.1 Load Raw Data](#41-load-raw-data)\n",
    "    3. [4.2 Instantiate data transformations](#42-instantiate-data-transformations)\n",
    "    4. [4.3 Create a Hugging Face Dataset](#43-create-a-hugging-face-dataset)\n",
    "    5. [4.4 Implement Data Collator](#44-implement-data-collator)\n",
    "5. [5. Model Setup](#5-model-setup)\n",
    "    1. [5.1 Apply LoRA](#51-apply-lora)\n",
    "    2. [5.2 Sequence-to-Sequence to Sequence-to-Score fine-tuning](#52-sequence-to-sequence-to-sequence-to-score-fine-tuning)\n",
    "        1. [5.2.1 Create a custom score head](#521-create-a-custom-score-head)\n",
    "        2. [5.2.2 Create a callback to automatically save the score head during training](#522-create-a-callback-to-automatically-save-the-score-head-during-training)\n",
    "        3. [5.2.3 Mock the forward method to replace the last layer with our score head](#523-mock-the-forward-method-to-replace-the-last-layer-with-our-score-head)\n",
    "6. [6. Training](#6-training)\n",
    "    1. [6.1 Define the training arguments and instantiate the trainer](#61-define-the-training-arguments-and-instantiate-the-trainer)\n",
    "    2. [6.2 Run the training loop](#62-run-the-training-loop)\n",
    "7. [7. Model Saving](#7-model-saving)\n",
    "8. [8. Inference](#8-inference)\n",
    "    1. [8.1 Load Model and Custom Score Module](#81-load-model-and-custom-score-module)\n",
    "    2. [8.2 Prepare Test Dataset](#82-prepare-test-dataset)\n",
    "    3. [8.3 Define the training arguments and instantiate the inference trainer](#83-define-the-training-arguments-and-instantiate-the-inference-trainer)\n",
    "    4. [8.4 Perform Inference](#84-perform-inference)\n",
    "9. [9. Conclusion](#9-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a09b0",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install the required packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86e137-e6ea-4a86-93ee-9f4469fc95d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Unsloth library (with CUDA 12.1 and Torch 2.3.0 support)\n",
    "# in vast.ai make sure that your template comes with torch 2.3.0 devel and CUDA 12.1\n",
    "# in vast.ai make sure you have enough disk storage (50gb+ for package installation + model download...)\n",
    "!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# Install additional dependencies\n",
    "!pip install scikit-learn\n",
    "!pip install wandb # optional for logging to wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587ae0d",
   "metadata": {},
   "source": [
    "Optional: set the WANDB_PROJECT environment variable for Weights & Biases tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a34aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_PROJECT=unsloth_lmsys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa327972-44a6-4ec0-b40b-fec48c01f10e",
   "metadata": {},
   "source": [
    "# 2. Data Transformations\n",
    "\n",
    "Define utility classes for processing and tokenizing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15337f",
   "metadata": {},
   "source": [
    "### 2.1 RawParser\n",
    "\n",
    "Transforms raw data from the competition datasets into \"samples\" (better suited for further processing and training).\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Input1[prompt: str] --> RawParser\n",
    "    Input2[response_a: str] --> RawParser\n",
    "    Input3[response_b: str] --> RawParser\n",
    "    RawParser --> Output1[prompt_ls: list[str]]\n",
    "    RawParser --> Output2[response_a_ls: list[str]]\n",
    "    RawParser --> Output3[response_b_ls: list[str]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beb1e8c4-f4a9-4cf9-913a-b05d6ff36a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, re, dataclasses as dc\n",
    "\n",
    "class RawParser:\n",
    "    \"\"\"transform a raw data point into a dataset sample\"\"\"\n",
    "    Out = dc.make_dataclass(\"Out\", [(\"prompt_ls\", list[str]), (\"response_a_ls\", list[str]), (\"response_b_ls\", list[str])])\n",
    "\n",
    "    @staticmethod\n",
    "    def recursive_match(s: str, pattern: str, matches: list[\"re.Match\"] = (), min_length: int = 1, offset: int = 0) -> list[\"re.Match\"]:\n",
    "        if len(s) - offset < min_length:\n",
    "            return matches\n",
    "\n",
    "        match = re.compile(pattern).search(s, offset)\n",
    "        if not match:\n",
    "            return matches\n",
    "    \n",
    "        offset = match.start() + 1\n",
    "        return RawParser.recursive_match(s, pattern, (*matches, match), min_length, offset)\n",
    "\n",
    "    @staticmethod\n",
    "    def null_to_none(s: str) -> str:\n",
    "        pattern = r\"((?:\\[|,)\\s*)(null)(\\s*(?:,|\\]))\"\n",
    "        for match in RawParser.recursive_match(s, pattern):\n",
    "            s = (\n",
    "                s[: match.start()]\n",
    "                + match.group().replace(\"null\", \"None\")\n",
    "                + s[match.end() :]\n",
    "            )\n",
    "        return s\n",
    "\n",
    "    def __call__(self, prompt: str, response_a: str, response_b: str) -> Out:\n",
    "        prompt_ls: list[str | None] = ast.literal_eval(self.null_to_none(prompt))\n",
    "        response_a_ls: list[str | None] = ast.literal_eval(self.null_to_none(response_a))\n",
    "        response_b_ls: list[str | None] = ast.literal_eval(self.null_to_none(response_b))\n",
    "         \n",
    "        def _to_str(ls: list[str | None]) -> list[str]:\n",
    "            t = []\n",
    "            for s in ls:\n",
    "                if not isinstance(s, str):\n",
    "                    t.append(str(s).replace(\"None\", \"\"))\n",
    "                else:\n",
    "                    # fix an issue with pyarrow not handling utf-16 surrogate pairs\n",
    "                    if bool(re.search(r\"[\\ud800-\\udfff]\", s)):\n",
    "                        s = s.encode(\"utf-16\", \"surrogatepass\").decode(\"utf-16\", \"surrogatepass\")\n",
    "                    t.append(s.strip())\n",
    "            return t\n",
    "\n",
    "        return self.Out(_to_str(prompt_ls), _to_str(response_a_ls), _to_str(response_b_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbd3f43",
   "metadata": {},
   "source": [
    "### 2.2 ParsedTokenizer\n",
    "\n",
    "Tokenizes each part of a sample into a list of input_ids.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Input1[prompt_ls: list[str]] --> ParsedTokenizer\n",
    "    Input2[response_a_ls: list[str]] --> ParsedTokenizer\n",
    "    Input3[response_b_ls: list[str]] --> ParsedTokenizer\n",
    "    ParsedTokenizer --> Output1[prompt_iid: list[list[int]]]\n",
    "    ParsedTokenizer --> Output2[response_a_iid: list[list[int]]]\n",
    "    ParsedTokenizer --> Output3[response_b_iid: list[list[int]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254b875b-1eeb-439c-ab58-86369bb0c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses as dc\n",
    "\n",
    "class ParsedTokenizer:\n",
    "    \"\"\"tokenize a sample into a list of input_ids\"\"\"\n",
    "    Out = dc.make_dataclass(\"Out\", [(\"prompt_iid\", list[list[int]]), (\"response_a_iid\", list[list[int]]), (\"response_b_iid\", list[list[int]])])\n",
    "\n",
    "    def __init__(self, tokenizer: \"transformers.FastTokenizer\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "    def __call__(self, prompt_ls: list[str], response_a_ls: list[str], response_b_ls: list[str]) -> Out:\n",
    "        prompt_encodings = self.tokenizer(prompt_ls, add_special_tokens=False)\n",
    "        response_a_encodings = self.tokenizer(response_a_ls, add_special_tokens=False)\n",
    "        response_b_encodings = self.tokenizer(response_b_ls, add_special_tokens=False)\n",
    "\n",
    "        return self.Out(prompt_encodings.input_ids, response_a_encodings.input_ids, response_b_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31bb8cf",
   "metadata": {},
   "source": [
    "### 2.3 SampleCreator\n",
    "\n",
    "Combines input_ids from the prompt(s), response_a(s) and response_b(s) into a single sample input_ids of **precisely** the desired size.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Input1[prompt_iid: list[list[int]]] --> SampleCreator\n",
    "    Input2[response_a_iid: list[list[int]]] --> SampleCreator\n",
    "    Input3[response_b_iid: list[list[int]]] --> SampleCreator\n",
    "    SampleCreator --> Output1[input_ids: list[int]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5d7d0c-973b-43e5-9c6b-b09cc745886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses as dc\n",
    "import textwrap\n",
    "\n",
    "class SampleCreator:\n",
    "    \"\"\"combines input_ids from the prompt(s), response_a(s) and response_b(s) into input_ids of the desired size\"\"\"\n",
    "    Out = dc.make_dataclass(\"Out\", [(\"input_ids\", list[int])])\n",
    "\n",
    "     # IMPORTANT: adapt special tokens and prompt format for each model's tokenizer.\n",
    "    SAMPLE_BEFORE_TURNS: str = textwrap.dedent(\"\"\"\n",
    "    <bos><start_of_turn>user\n",
    "    Answer \"tie\" if both model's answers are similar. Answer \"A\" if model A's answers are better, or \"B\" if model B's answers are better.<end_of_turn>\n",
    "    \"\"\").strip(\"\\n\")\n",
    "    TURN_BEFORE_PROMPT: str =  \"<start_of_turn>user\\n\"\n",
    "    TURN_AFTER_PROMPT: str = \"<end_of_turn><start_of_turn>model A\\n\"\n",
    "    TURN_AFTER_RESPONSE_A: str = \"<end_of_turn><start_of_turn>model B\\n\"\n",
    "    TURN_AFTER_RESPONSE_B: str = \"<end_of_turn>\"\n",
    "    SAMPLE_AFTER_TURNS: str = \"<start_of_turn>model\\n\"\n",
    "    \n",
    "\n",
    "    def __init__(self, tokenizer, max_len: int = 1024, min_turn_content_len: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_turn_content_len = min_turn_content_len\n",
    "\n",
    "        self.sample_before_turns_ids = self.tokenizer(self.SAMPLE_BEFORE_TURNS, add_special_tokens=False).input_ids\n",
    "        self.turn_before_prompt_ids = self.tokenizer(self.TURN_BEFORE_PROMPT, add_special_tokens=False).input_ids\n",
    "        self.turn_after_prompt_ids = self.tokenizer(self.TURN_AFTER_PROMPT, add_special_tokens=False).input_ids\n",
    "        self.turn_after_response_a_ids = self.tokenizer(self.TURN_AFTER_RESPONSE_A, add_special_tokens=False).input_ids\n",
    "        self.turn_after_response_b_ids = self.tokenizer(self.TURN_AFTER_RESPONSE_B, add_special_tokens=False).input_ids\n",
    "        self.sample_after_turns_ids = self.tokenizer(self.SAMPLE_AFTER_TURNS, add_special_tokens=False).input_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def _greedy_reduce_to_threshold(values: list[int], threshold: int) -> list[int]:\n",
    "        while sum(values) > threshold:\n",
    "            max_index = values.index(max(values))\n",
    "            values[max_index] -= 1\n",
    "        return values\n",
    "    \n",
    "    @staticmethod\n",
    "    def _truncate_sequence(seq: list[int], max_len: int, ellipsis_id: int) -> list[int]:\n",
    "        if len(seq) <= max_len:\n",
    "            return seq\n",
    "        return seq[:max_len-1] + [ellipsis_id]\n",
    "    \n",
    "    def __call__(self, prompt_iid: list[list[int]], response_a_iid: list[list[int]], response_b_iid: list[list[int]]) -> Out:\n",
    "        special_tokens_per_conversation = len(self.sample_before_turns_ids) + len(self.sample_after_turns_ids)\n",
    "        special_tokens_per_turn = len(self.turn_before_prompt_ids) + len(self.turn_after_prompt_ids) + len(self.turn_after_response_a_ids) + len(self.turn_after_response_b_ids)\n",
    "        ellipsis_id = self.tokenizer(\"...\", add_special_tokens=False).input_ids[0]\n",
    "        content_max_len_left = self.max_len - special_tokens_per_conversation\n",
    "\n",
    "        turns = []\n",
    "        for turn in range(len(prompt_iid)):\n",
    "            content_max_len_left -= special_tokens_per_turn\n",
    "\n",
    "            if content_max_len_left < self.min_turn_content_len:\n",
    "                break\n",
    "    \n",
    "            prompt_len, response_a_len, response_b_len = map(len, [\n",
    "                prompt_iid[turn],\n",
    "                response_a_iid[turn],\n",
    "                response_b_iid[turn]\n",
    "            ])\n",
    "            \n",
    "            sizes = self._greedy_reduce_to_threshold([prompt_len, response_a_len, response_b_len], content_max_len_left)\n",
    "            \n",
    "            turn_data = {\n",
    "                \"prompt\": self._truncate_sequence(prompt_iid[turn], sizes[0], ellipsis_id),\n",
    "                \"response_a\": self._truncate_sequence(response_a_iid[turn], sizes[1], ellipsis_id),\n",
    "                \"response_b\": self._truncate_sequence(response_b_iid[turn], sizes[2], ellipsis_id)\n",
    "            }\n",
    "\n",
    "            \n",
    "            content_max_len_left -= sum(sizes)\n",
    "            turns.append(turn_data)\n",
    "\n",
    "        input_ids = []\n",
    "        input_ids.extend(self.sample_before_turns_ids)\n",
    "        for turn in turns:\n",
    "            input_ids.extend(self.turn_before_prompt_ids)\n",
    "            input_ids.extend(turn[\"prompt\"])\n",
    "            input_ids.extend(self.turn_after_prompt_ids)\n",
    "            input_ids.extend(turn[\"response_a\"])\n",
    "            input_ids.extend(self.turn_after_response_a_ids)\n",
    "            input_ids.extend(turn[\"response_b\"])\n",
    "            input_ids.extend(self.turn_after_response_b_ids)\n",
    "        input_ids.extend(self.sample_after_turns_ids)\n",
    "\n",
    "        return self.Out(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762bb06",
   "metadata": {},
   "source": [
    "### 2.4 LabelCreator\n",
    "\n",
    "Creates labels from raw labels (label input_ids).\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    Input1[winner_model_a: int] --> LabelCreator\n",
    "    Input2[winner_model_b: int] --> LabelCreator\n",
    "    Input3[winner_tie: int] --> LabelCreator\n",
    "    LabelCreator --> Output1[label_ids: int]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36bee245-e4e6-4206-8377-ac63fb1c58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses as dc\n",
    "\n",
    "class LabelCreator:\n",
    "    \"\"\"create LLM labels from raw labels (label input_ids)\"\"\"\n",
    "    def __init__(self, tokenizer, win_a_word: str = \"A\", win_b_word: str = \"B\", win_tie_word: str = \"tie\"):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.win_a_word = \"A\"\n",
    "        self.win_b_word = \"B\"\n",
    "        self.win_tie_word = \"tie\"\n",
    "\n",
    "        self.label2id = {k: self.tokenizer(k, add_special_tokens=False).input_ids[0] for k in [win_a_word, win_b_word, win_tie_word]}\n",
    "        self.id2label = {v: k for k, v in self.label2id.items()}\n",
    "\n",
    "    def __call__(self, winner_model_a: int, winner_model_b: int, winner_tie: int) -> int:\n",
    "        assert sum([winner_model_a, winner_model_b, winner_tie]) == 1\n",
    "        if winner_model_a == 1:\n",
    "            return self.label2id[self.win_a_word]\n",
    "\n",
    "        if winner_model_b == 1:\n",
    "            return self.label2id[self.win_b_word]\n",
    "\n",
    "        if winner_tie == 1:\n",
    "            return self.label2id[self.win_tie_word]\n",
    "\n",
    "        raise AttributeError(\"label should be winner a, winner b or tie but none were set to 1...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586bb7f-7c29-42c0-bcd9-f27f505c3d5f",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set the configuration parameters for the model, training, and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9baaa81f-00b0-4ffe-b4e1-907f2072c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "MAX_SEQ_LEN = 2048 # sample maximum seq_len to use for training\n",
    "NUM_PROC = 12 # number of processes to use for data processing\n",
    "SEED = 42 # seed for reproducibility\n",
    "\n",
    "# MODEL\n",
    "MODEL_PATH = \"unsloth/gemma-2-9b-it-bnb-4bit\" # hugging face model id to use for training\n",
    "LORA_R = 16 # LoRA rank\n",
    "LORA_ALPHA = 2 * LORA_R # LoRA alpha\n",
    "LORA_DROPOUT = 0 # LoRA dropout\n",
    "LORA_BIAS = \"none\" # LoRA bias (none, all, or per_module)\n",
    "LORA_TGT_MOD = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # LoRA target modules\n",
    "LORA_SEED = SEED # LoRA seed \n",
    "LORA_GRAD_CHKPOINT = \"unsloth\" # LoRA gradient checkpointing    \n",
    "LORA_RSLORA = False # LoRA rank stabilized\n",
    "LORA_LOFTQ_CFG = None # LoRA LoftQ config\n",
    "\n",
    "# TRAINING\n",
    "TR_OUTPUT_DIR = \"36_unsloth_gemma2_9b_2048_1epochs_1e-4\" # Training output directory\n",
    "TR_SAVE_STEPS = 200 # Save steps\n",
    "TR_WARMUP_STEPS = 0 # Warmup steps\n",
    "TR_OPTIM = \"adamw_8bit\" # Optimizer\n",
    "TR_LR = 1e-4 # Learning rate\n",
    "TR_BSZ = 16 # Training batch size\n",
    "TR_EVAL_BSZ = 6 # Evaluation batch size\n",
    "TR_GRAD_ACC = 1 # Gradient accumulation steps\n",
    "TR_EPOCHS = 1 # Number of epochs\n",
    "\n",
    "# DATA\n",
    "TRAIN_CSV_PATH = \"data/train.csv\" # Training data path (this is the csv from kaggle)\n",
    "TEST_SIZE=0.2 # Test size\n",
    "DS_SEED=SEED # Dataset seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa419e",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "Load the competition dataset, use the data transformations defined above to prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f864bf",
   "metadata": {},
   "source": [
    "### 4.0 Load the model and tokenizer\n",
    "\n",
    "The tokenizer is necessary to instantiate our data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1e4a6e-09db-4a50-bafe-0a4adef7070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.643 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff32474456a446cb889999710476bd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e608bd82ea4907a212ca7bb6e3667e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2644f97ec7a44b1a99f603d18eb3e37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fbb061ae8e414f8a2f4774ba74c143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dae50c77cd1432bb2b95c02bb1ca373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9ba7d9dabe4671ae3e03dde1e1051d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_PATH,\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac633b",
   "metadata": {},
   "source": [
    "### 4.1 Load Raw Data\n",
    "\n",
    "Load the raw competition training dataset as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc54cfd-8253-46f5-991c-e56090155d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57477, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             model_a              model_b  \\\n",
       "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1   53567           koala-13b           gpt-4-0613   \n",
       "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "1  [\"What is the difference between marriage lice...   \n",
       "2  [\"explain function calling. how would you call...   \n",
       "3  [\"How can I create a test set for a very rare ...   \n",
       "4  [\"What is the best way to travel from Tel-Aviv...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "1  [\"A marriage license is a legal document that ...   \n",
       "2  [\"Function calling is the process of invoking ...   \n",
       "3  [\"Creating a test set for a very rare category...   \n",
       "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1  [\"A marriage license and a marriage certificat...               0   \n",
       "2  [\"Function calling is the process of invoking ...               0   \n",
       "3  [\"When building a classifier for a very rare c...               1   \n",
       "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  \n",
       "1               1           0  \n",
       "2               0           1  \n",
       "3               0           0  \n",
       "4               1           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e1723",
   "metadata": {},
   "source": [
    "### 4.2 Instantiate data transformations\n",
    "\n",
    "Create instances of the data processing classes defined earlier with the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9237c62a-3555-4be9-b2e6-924048b3b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate data processing classes\n",
    "raw_parser = RawParser()\n",
    "parsed_tokenizer = ParsedTokenizer(tokenizer)\n",
    "sample_creator = SampleCreator(tokenizer, MAX_SEQ_LEN)\n",
    "label_creator = LabelCreator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5cfbe",
   "metadata": {},
   "source": [
    "## 4.3 Create a Hugging Face Dataset\n",
    "\n",
    "We use a hugging face dataset for processing the dataset as it allows to quickly parallelize the job on all CPU threads available while maintaining low storage requirements with pyarrow. The added benefit is that we can directly use the dataset in the hugging face trainer later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb52dcac-414d-4b97-8b7d-595fb97fa4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44acb45b0b2241da8eab41124abdbca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/57477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def map_fn(samples: dict[str, list[any]]) -> dict[str, list[any]]:\n",
    "    return_dict = {\"input_ids\": [], \"label_ids\": [], \"seq_len\": []}\n",
    "\n",
    "    for prompt, response_a, response_b, winner_model_a, winner_model_b, winner_tie in zip(\n",
    "            samples[\"prompt\"], samples[\"response_a\"], samples[\"response_b\"], samples[\"winner_model_a\"], samples[\"winner_model_b\"], samples[\"winner_tie\"]):\n",
    "    \n",
    "            out = raw_parser(prompt, response_a, response_b)\n",
    "            out = parsed_tokenizer(out.prompt_ls, out.response_a_ls, out.response_b_ls)\n",
    "            out = sample_creator(out.prompt_iid, out.response_a_iid, out.response_b_iid)\n",
    "    \n",
    "            label_ids = label_creator(winner_model_a, winner_model_b, winner_tie)\n",
    "            \n",
    "            return_dict[\"input_ids\"].append(out.input_ids)\n",
    "            return_dict[\"label_ids\"].append(label_ids)\n",
    "            return_dict[\"seq_len\"].append(len(out.input_ids)) # enable groupbylength sampling\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "dataset = (\n",
    "    Dataset.from_pandas(df)\n",
    "    .select_columns([\"id\", \"prompt\", \"response_a\", \"response_b\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"])\n",
    "    .map(map_fn, batched=True, num_proc=NUM_PROC)\n",
    "    # .train_test_split(test_size=TEST_SIZE, seed=DS_SEED)\n",
    "    .select_columns([\"id\", \"input_ids\", \"label_ids\", \"seq_len\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4ad64",
   "metadata": {},
   "source": [
    "### 4.4 Implement Data Collator\n",
    "\n",
    "Define a data collator for batching samples during training. The main reason to use a custom collator here is that we delay padding to the very end when samples are already batched to optimize batch sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "381d5221-1d20-4090-b278-0503e0bef234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "\n",
    "    def __call__(self, samples: list[dict[str, any]]) -> dict[str, any]:\n",
    "        # flatten into a single dict as expected by tokenizer.pad\n",
    "        d = {key: [] for key in samples[0].keys()}\n",
    "        for sample in samples:\n",
    "            for k, v in sample.items():\n",
    "                d[k].append(v)\n",
    "\n",
    "        encodings = tokenizer.pad({\"input_ids\": d[\"input_ids\"]}, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"id\": d[\"id\"],\n",
    "            \"input_ids\": encodings.input_ids,\n",
    "            \"attention_mask\": encodings.attention_mask,\n",
    "            \"labels\": torch.tensor(d[\"label_ids\"])\n",
    "        }\n",
    "\n",
    "collator_fn = Collator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e95823",
   "metadata": {},
   "source": [
    "## 5. Model Setup\n",
    "\n",
    "Load the pre-trained model and prepare it for training with QLoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ce818",
   "metadata": {},
   "source": [
    "### 5.1 Apply LoRA\n",
    "\n",
    "Unsloth patch the \"peft\" model to make it run faster during fine-tuning. Optimized kernels for matrix multiplication, reorder operations, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6672cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_R,\n",
    "    target_modules = LORA_TGT_MOD,\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = LORA_DROPOUT, # Supports any, but = 0 is optimized\n",
    "    bias = LORA_BIAS,    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = LORA_GRAD_CHKPOINT, # True or \"unsloth\" for very long context\n",
    "    random_state = LORA_SEED,\n",
    "    use_rslora = LORA_RSLORA,  # We support rank stabilized LoRA\n",
    "    loftq_config = LORA_LOFTQ_CFG, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2a4159",
   "metadata": {},
   "source": [
    "### 5.2 Sequence-to-Sequence to Sequence-to-Score fine-tuning\n",
    "\n",
    "By default Unsloth only supports sequence to sequence model fine tuning. In this problem it is more efficient to replace the last layer of the seq-to-seq model (a classifier over the full vocabulary) with a simple classifier over the 3 possible labels (A, B or tie).\n",
    "\n",
    "To that effect we need to perform 3 steps: \n",
    "1. create a custom score head in full precision (for better performance it won't be trained with QLoRA)\n",
    "2. save it to the model folder so it can be loaded during inference using a callback\n",
    "3. mock the forward method to replace the last layer with our score head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335783fe",
   "metadata": {},
   "source": [
    "#### 5.2.1 Create a custom score head\n",
    "\n",
    "We make sure to create the score head in full precision to avoid any accuracy loss during training. Also this layer will be trained normally (without QLoRA).\n",
    "\n",
    "Note that it might be optimal to use 2 optimizers in this case to have more control over the learning rate of the score head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c351cab-fbc3-4838-9d48-21ed89fae155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our own head in full precision\n",
    "import torch\n",
    "\n",
    "score = torch.nn.Linear(model.lm_head.in_features, 3, bias=False, dtype=torch.float32, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2e7a9",
   "metadata": {},
   "source": [
    "#### 5.2.2 Create a callback to automatically save the score head during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5a56a9b-523c-4380-88e7-b0b99754d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class ScoreSaverCallback(TrainerCallback):\n",
    "    def __init__(self, score_module):\n",
    "        self.score_module = score_module\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        checkpoint_folder = f\"checkpoint-{state.global_step}\"\n",
    "        output_dir = os.path.join(args.output_dir, checkpoint_folder)\n",
    "        score_path = os.path.join(output_dir, \"score.pth\")\n",
    "        \n",
    "        torch.save(self.score_module.state_dict(), score_path)\n",
    "        print(f\"Saved score module to {score_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c488b3",
   "metadata": {},
   "source": [
    "#### 5.2.3 Mock the forward method to replace the last layer with our score head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60438959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_forward(\n",
    "    self,\n",
    "    input_ids = None,\n",
    "    causal_mask= None,\n",
    "    attention_mask = None,\n",
    "    position_ids = None,\n",
    "    past_key_values= None,\n",
    "    inputs_embeds = None,\n",
    "    labels = None,\n",
    "    use_cache = None,\n",
    "    output_attentions = None,\n",
    "    output_hidden_states = None,\n",
    "    return_dict = None,\n",
    "    *args, **kwargs,\n",
    "):\n",
    "    causal_mask = xformers.attn_bias.LowerTriangularMask()\n",
    "\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    self.model._has_no_labels = labels is None\n",
    "\n",
    "    outputs = self.model(\n",
    "        input_ids=input_ids,\n",
    "        causal_mask=causal_mask,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "\n",
    "    hidden_states = outputs[0]\n",
    "    bsz, q_len, hd = hidden_states.shape\n",
    "\n",
    "    # pool the hidden_states by getting only last for each seq\n",
    "    seq_idx = attention_mask.sum(-1) - 1 # (bsz,)\n",
    "    hidden_states = hidden_states[torch.arange(bsz), seq_idx, :].unsqueeze(1) # (bsz, 1, hd)\n",
    "    \n",
    "    # lm_head = self.lm_head.weight\n",
    "    # logits = self.lm_head(hidden_states.to(lm_head.dtype)) # (bsz, 1, vocab)\n",
    "    # logits = logits.to(self.config.torch_dtype) \n",
    "\n",
    "    loss = None\n",
    "    logit_softcapping = getattr(self.config, \"final_logit_softcapping\", 0)\n",
    "\n",
    "    # shift_logits = logits[..., list(label_creator.id2label.keys())] # (bsz, 1, 3)\n",
    "    shift_logits = score(hidden_states.to(score.weight.dtype)) # (bsz, 1, 3)\n",
    "    shift_labels = labels.unsqueeze(-1) # (bsz, 1)\n",
    "    for i, label_id in enumerate(label_creator.id2label.keys()):\n",
    "        shift_labels[shift_labels == label_id] = i\n",
    "\n",
    "    loss = fast_cross_entropy_loss(\n",
    "        logits = shift_logits,\n",
    "        labels = shift_labels,\n",
    "        logit_softcapping = logit_softcapping,\n",
    "    )\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (shift_logits,) + outputs[1:]\n",
    "        return (loss,) + output if loss is not None else output\n",
    "\n",
    "    return CausalLMOutputWithPast(\n",
    "        loss=loss,\n",
    "        logits=shift_logits,\n",
    "        past_key_values=outputs.past_key_values,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b7b32",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Set up the training arguments, instantiate the trainer and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09901fc8",
   "metadata": {},
   "source": [
    "### 6.1 Define the training arguments and instantiate the trainer\n",
    "\n",
    "**Important Notes**\n",
    "- group by length combined with our custom collator allows for much faster training as batches can be grouped by length and padded to the maximum sequence length of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dcdd566-c093-443d-9537-7fea70c19a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TRAINER\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=TR_OUTPUT_DIR,\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=TR_SAVE_STEPS,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=TR_WARMUP_STEPS,\n",
    "    optim=TR_OPTIM,\n",
    "    learning_rate=TR_LR,\n",
    "    per_device_train_batch_size=TR_BSZ,\n",
    "    per_device_eval_batch_size=TR_EVAL_BSZ,\n",
    "    gradient_accumulation_steps=TR_GRAD_ACC,\n",
    "    num_train_epochs=TR_EPOCHS,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=TR_OUTPUT_DIR,\n",
    "    remove_unused_columns=False, # don't remove id column...\n",
    "    group_by_length=True, # group by length for faster training\n",
    "    length_column_name=\"seq_len\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator_fn,\n",
    "    callbacks=[ScoreSaverCallback(score_module=score)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6ce64",
   "metadata": {},
   "source": [
    "### 6.2 Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ca3174-7540-4724-9588-5efa83637065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 57,477 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 16 | Total steps = 3,593\n",
      " \"-____-\"     Number of trainable parameters = 54,018,048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguillaume-raille\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240804_015804-j32za0ws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guillaume-raille/unsloth_lmsys/runs/j32za0ws' target=\"_blank\">36_unsloth_gemma2_9b_2048_1epochs_1e-4</a></strong> to <a href='https://wandb.ai/guillaume-raille/unsloth_lmsys' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guillaume-raille/unsloth_lmsys' target=\"_blank\">https://wandb.ai/guillaume-raille/unsloth_lmsys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guillaume-raille/unsloth_lmsys/runs/j32za0ws' target=\"_blank\">https://wandb.ai/guillaume-raille/unsloth_lmsys/runs/j32za0ws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3593' max='3593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3593/3593 5:15:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.076900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.083400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.976700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.939100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.988500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.103100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.951300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.957600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.984800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.996900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.818700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.969200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.991900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.884100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.949400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.953100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.894400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.919800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.919600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.963400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.850900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.961400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.973800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.919800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.968500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.920300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.925500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.822300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.924600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.899900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.958900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.936900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.791600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.940400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.922200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.791900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>1.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.868200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.786100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>1.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.793700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.860100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.795700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.936200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.937900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.770600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.820600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.942600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.950400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.964200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.965500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.874700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.870800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.872100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.898600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.994500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>0.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.901600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>0.927500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>0.896300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>0.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>0.952700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.973100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>0.868600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>0.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>0.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>0.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>0.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>0.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>1.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>0.813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>0.988600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>0.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>0.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>0.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>0.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>1.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>0.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.760500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>0.948100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.983200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>0.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.915900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>0.958500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>0.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.731700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>0.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>0.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.934900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.969900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>0.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-200/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-400/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-600/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-800/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-1000/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-1200/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-1400/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-1600/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-1800/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-2000/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-2200/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-2400/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-2600/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-2800/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-3000/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-3200/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-3400/score.pth\n",
      "Saved score module to 36_unsloth_gemma2_9b_2048_1epochs_1e-4/checkpoint-3593/score.pth\n"
     ]
    }
   ],
   "source": [
    "from unittest import mock\n",
    "\n",
    "with mock.patch.object(model.base_model.model,'forward', new=mock_forward.__get__(model.base_model.model, type(model.base_model.model))):\n",
    "    trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2cc2bf",
   "metadata": {},
   "source": [
    "## 7. Model Saving\n",
    "\n",
    "Save the trained model, tokenizer and custom score module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3db1012e-9cc9-4e62-8eff-9e3957a8e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "model.save_pretrained(f\"{TR_OUTPUT_DIR}/final\") # Local saving\n",
    "tokenizer.save_pretrained(f\"{TR_OUTPUT_DIR}/final\")\n",
    "torch.save(score, f\"{TR_OUTPUT_DIR}/final/score.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b986647",
   "metadata": {},
   "source": [
    "Finish the Weights & Biases run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fff7ff61-d127-446a-88aa-021e9a4b48fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▅▃▃▃▁▂▃▂▂▂▃▂▃▂▁▃▂▃▂▃▃▃▄▄▄▃▄▁▄▃▅▃▂▂▃▃▃▄▂</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇██▅▄▆█▃▇▄▅▃▅▂▆▇▂▄▃▆▄▅▄▂▁▄▅▂▆▃▅▁▅▁▂▄▁▆▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>2.29905159755469e+18</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>3593</td></tr><tr><td>train/grad_norm</td><td>2.81213</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7639</td></tr><tr><td>train_loss</td><td>0.94177</td></tr><tr><td>train_runtime</td><td>18974.4169</td></tr><tr><td>train_samples_per_second</td><td>3.029</td></tr><tr><td>train_steps_per_second</td><td>0.189</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">36_unsloth_gemma2_9b_2048_1epochs_1e-4</strong> at: <a href='https://wandb.ai/guillaume-raille/unsloth_lmsys/runs/j32za0ws' target=\"_blank\">https://wandb.ai/guillaume-raille/unsloth_lmsys/runs/j32za0ws</a><br/> View project at: <a href='https://wandb.ai/guillaume-raille/unsloth_lmsys' target=\"_blank\">https://wandb.ai/guillaume-raille/unsloth_lmsys</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240804_015804-j32za0ws/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51314973-38c9-4eb2-ba00-dd67ddcc65a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Inference\n",
    "\n",
    "Note that this is not the inference script used in the competition but still useful to quickly test if the fine tuned model is working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11843cee",
   "metadata": {},
   "source": [
    "### 8.1 Load Model and Custom Score Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6844cc-dac2-4a13-84f6-01343c09857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.643 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL & TOKENIZER\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"32_unsloth_gemma2_9b_2048\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab79ad",
   "metadata": {},
   "source": [
    "### 8.2 Prepare Test Dataset\n",
    "\n",
    "This time we split into training and testing set while before we took the whole dataset as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ffda0d-dda4-4514-88ee-7865e49b8fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24a4aab7071457784d9ce9bcd91a2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/57477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE HF DATASET for TRAINING / TESTING\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = (\n",
    "    Dataset.from_pandas(df)\n",
    "    .select_columns([\"id\", \"prompt\", \"response_a\", \"response_b\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"])\n",
    "    .map(map_fn, batched=True, num_proc=NUM_PROC)\n",
    "    .train_test_split(test_size=TEST_SIZE, seed=DS_SEED)\n",
    "    .select_columns([\"id\", \"input_ids\", \"label_ids\", \"seq_len\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128258b",
   "metadata": {},
   "source": [
    "### 8.3 Define the training arguments and instantiate the inference trainer\n",
    "\n",
    "Our inference trainer is essentially the same as our training trainer, instead of using the train method we will use the predict method. Note that because our test set is in our training set we expect here quite high accuracy (much higher than on the competition private dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06a0edfd-ce15-45da-9407-906b3645ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TRAINER\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=TR_OUTPUT_DIR,\n",
    "    overwrite_output_dir = True,\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=TR_SAVE_STEPS,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=TR_WARMUP_STEPS,\n",
    "    optim=TR_OPTIM,\n",
    "    learning_rate=TR_LR,\n",
    "    per_device_train_batch_size=TR_BSZ,\n",
    "    per_device_eval_batch_size=TR_EVAL_BSZ,\n",
    "    gradient_accumulation_steps=TR_GRAD_ACC,\n",
    "    num_train_epochs=TR_EPOCHS,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False, # don't remove id column...\n",
    "    group_by_length=True, # group by length for faster training\n",
    "    length_column_name=\"seq_len\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268bbf07",
   "metadata": {},
   "source": [
    "### 8.4 Perform Inference\n",
    "\n",
    "We load the score module and mock the forward method to replace the last layer with our score head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88930413-3f0c-427f-97d8-9cafb10b7865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ACTUAL INFERENCE LOOP\n",
    "from unittest import mock\n",
    "\n",
    "from unsloth.models.llama import xformers, CausalLMOutputWithPast, fast_cross_entropy_loss\n",
    "import torch\n",
    "\n",
    "# load my own head in full precision\n",
    "score = torch.load(\"32_unsloth_gemma2_9b_2048/score.pth\")\n",
    "\n",
    "# unfortunately we have to redefine the mock_forward because we pass load and pass the score module\n",
    "# we could refactor the code to avoid this.\n",
    "def mock_forward(\n",
    "    self,\n",
    "    input_ids = None,\n",
    "    causal_mask= None,\n",
    "    attention_mask = None,\n",
    "    position_ids = None,\n",
    "    past_key_values= None,\n",
    "    inputs_embeds = None,\n",
    "    labels = None,\n",
    "    use_cache = None,\n",
    "    output_attentions = None,\n",
    "    output_hidden_states = None,\n",
    "    return_dict = None,\n",
    "    *args, **kwargs,\n",
    "):\n",
    "    causal_mask = xformers.attn_bias.LowerTriangularMask()\n",
    "\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    self.model._has_no_labels = labels is None\n",
    "\n",
    "    outputs = self.model(\n",
    "        input_ids=input_ids,\n",
    "        causal_mask=causal_mask,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "\n",
    "    hidden_states = outputs[0]\n",
    "    bsz, q_len, hd = hidden_states.shape\n",
    "\n",
    "    # pool the hidden_states by getting only last for each seq\n",
    "    seq_idx = attention_mask.sum(-1) - 1 # (bsz,)\n",
    "    hidden_states = hidden_states[torch.arange(bsz), seq_idx, :].unsqueeze(1) # (bsz, 1, hd)\n",
    "\n",
    "    loss = None\n",
    "    logit_softcapping = getattr(self.config, \"final_logit_softcapping\", 0)\n",
    "\n",
    "    shift_logits = score(hidden_states.to(score.weight.dtype)) # (bsz, 1, 3)\n",
    "    shift_labels = labels.unsqueeze(-1) # (bsz, 1)\n",
    "    for i, label_id in enumerate(label_creator.id2label.keys()):\n",
    "        shift_labels[shift_labels == label_id] = i\n",
    "\n",
    "    loss = fast_cross_entropy_loss(\n",
    "        logits = shift_logits,\n",
    "        labels = shift_labels,\n",
    "        logit_softcapping = logit_softcapping,\n",
    "    )\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (shift_logits,) + outputs[1:]\n",
    "        return (loss,) + output if loss is not None else output\n",
    "\n",
    "    return CausalLMOutputWithPast(\n",
    "        loss=loss,\n",
    "        logits=shift_logits,\n",
    "    )\n",
    "\n",
    "with mock.patch.object(model.base_model.model,'forward', new=mock_forward.__get__(model.base_model.model, type(model.base_model.model))):\n",
    "    prediction_outputs = trainer.predict(dataset[\"test\"].sort(\"seq_len\", reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81549b48-36c8-4bc4-9d87-739382f80643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8944498896598816,\n",
       " 'eval_model_preparation_time': 0.0143,\n",
       " 'eval_runtime': 1205.585,\n",
       " 'eval_samples_per_second': 9.536,\n",
       " 'eval_steps_per_second': 1.589}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dcdbe0",
   "metadata": {},
   "source": [
    "Note that in this case we obtained a negative log-likelihood of 0.894 which is extremly good on this problem. As we have 3 classes random would be $-\\ln(\\frac{1}{3}) = 1.0986$. Again this is because the test set is in the training dataset but at least we have a sense that the fine-tuning somehow worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24392952",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This concludes our notebook on finetuning gemma 2 9B for sequence classification to predict human preference. We have seen how to use Unsloth to quickly and efficiently fine-tune a model with QLoRA and how to implement a custom score head for converting the seq-to-seq into seq-to-cls model. We have also seen how to use the fast tokenizer and data processing pipeline to quickly create a custom dataset and dataloader.\n",
    "\n",
    "In the next notebook we will see how to quantize the final model while retaining good accuracy with autoAWQ. We will then use the quantized model in the context of the competition for inference by customizing VLLM for sequence classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
